{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Desktop\\Malmo\\MalmoPlatform\\Schemas\n"
     ]
    }
   ],
   "source": [
    "import malmo.minecraftbootstrap; #malmo.minecraftbootstrap.launch_minecraft()\n",
    "malmo.minecraftbootstrap.set_malmo_xsd_path()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from future import standard_library\n",
    "standard_library.install_aliases()\n",
    "from builtins import range\n",
    "from builtins import object\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import json\n",
    "import random\n",
    "import logging\n",
    "\n",
    "# Microsoft Malmo API\n",
    "import malmo.MalmoPython as MalmoPython\n",
    "\n",
    "# Check Tkinter version on your pc.\n",
    "if sys.version_info[0] == 2:  \n",
    "    import Tkinter as tk\n",
    "else:\n",
    "    import tkinter as tk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define agent's action\n",
    "agent_actions = [\"move 1\", \"move -1\", \"turn 1\", \"turn -1\"]\n",
    "\n",
    "# Appoint value to epsilon.\n",
    "#epsilon = 0.0\n",
    "#epsilon = 0.1\n",
    "#epsilon = 0.2\n",
    "#epsilon = 0.3\n",
    "#epsilon = 0.4\n",
    "#epsilon = 0.5\n",
    "#epsilon = 0.6\n",
    "epsilon = 0.7\n",
    "#epsilon = 0.8\n",
    "#epsilon = 0.9\n",
    "#epsilon = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GreedyEpsilonQL(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.epsilon = epsilon \n",
    "        self.count = 0\n",
    "\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "        if False:\n",
    "            self.logger.setLevel(logging.DEBUG)\n",
    "        else:\n",
    "            self.logger.setLevel(logging.INFO)\n",
    "        self.logger.handlers = []\n",
    "        self.logger.addHandler(logging.StreamHandler(sys.stdout))\n",
    "\n",
    "        self.actions = agent_actions\n",
    "        self.q_table = {}\n",
    "        self.canvas = None\n",
    "        self.root = None\n",
    "        \n",
    "    def count_action(self):\n",
    "        return self.count\n",
    "    \n",
    "    def resetcount_action(self):\n",
    "        self.count = 0\n",
    "        return self.count\n",
    "\n",
    "    def updateQTable( self, reward, current_state ):\n",
    "        \n",
    "        # retrieve the old action value from the Q-table (indexed by the previous state and the previous action)\n",
    "        old_q = self.q_table[self.prev_s][self.prev_a]\n",
    "        \n",
    "        new_q = reward\n",
    "        \n",
    "        # assign the new action value to the Q-table\n",
    "        self.q_table[self.prev_s][self.prev_a] = new_q\n",
    "        \n",
    "    def updateQTableFromTerminatingState( self, reward ):\n",
    "        \n",
    "        # retrieve the old action value from the Q-table (indexed by the previous state and the previous action)\n",
    "        old_q = self.q_table[self.prev_s][self.prev_a]\n",
    "        \n",
    "        new_q = reward\n",
    "        \n",
    "        # assign the new action value to the Q-table\n",
    "        self.q_table[self.prev_s][self.prev_a] = new_q\n",
    "        \n",
    "    def act(self, world_state, agent_host, current_r ):\n",
    "        \n",
    "        obs_text = world_state.observations[-1].text\n",
    "        obs = json.loads(obs_text) # most recent observation\n",
    "        self.logger.debug(obs)\n",
    "        if not u'XPos' in obs or not u'ZPos' in obs:\n",
    "            self.logger.error(\"Incomplete observation received: %s\" % obs_text)\n",
    "            return 0\n",
    "        current_s = \"%d:%d\" % (int(obs[u'XPos']), int(obs[u'ZPos']))\n",
    "        self.logger.debug(\"State: %s (x = %.2f, z = %.2f)\" % (current_s, float(obs[u'XPos']), float(obs[u'ZPos'])))\n",
    "        if current_s not in self.q_table:\n",
    "            self.q_table[current_s] = ([0] * len(self.actions))\n",
    "\n",
    "        # update Q values\n",
    "        if self.prev_s is not None and self.prev_a is not None:\n",
    "            self.updateQTable( current_r, current_s )\n",
    "\n",
    "        self.drawQ( curr_x = int(obs[u'XPos']), curr_y = int(obs[u'ZPos']) )\n",
    "\n",
    "        # select the next action\n",
    "        rnd = random.random()\n",
    "        if rnd < self.epsilon:\n",
    "            a = random.randint(0, len(self.actions) - 1)\n",
    "            self.actions[a]\n",
    "        else:\n",
    "            m = max(self.q_table[current_s])\n",
    "            self.logger.debug(\"Current values: %s\" % \",\".join(str(x) for x in self.q_table[current_s]))\n",
    "            l = list()\n",
    "            for x in range(0, len(self.actions)):\n",
    "                if self.q_table[current_s][x] == m:\n",
    "                    l.append(x)\n",
    "            y = random.randint(0, len(l)-1)\n",
    "            a = l[y]\n",
    "            self.actions[a]\n",
    "        \n",
    "        # try to send the selected action, only update prev_s if this succeeds\n",
    "        try:\n",
    "            agent_host.sendCommand(self.actions[a])\n",
    "            self.prev_s = current_s\n",
    "            self.prev_a = a\n",
    "\n",
    "        except RuntimeError as e:\n",
    "            self.logger.error(\"Failed to send command: %s\" % e)\n",
    "        \n",
    "        self.count += 1\n",
    "        return current_r\n",
    "\n",
    "    def run(self, agent_host):\n",
    "\n",
    "        total_reward = 0\n",
    "        \n",
    "        self.prev_s = None\n",
    "        self.prev_a = None\n",
    "        \n",
    "        is_first_action = True\n",
    "        \n",
    "        # main loop:\n",
    "        world_state = agent_host.getWorldState()\n",
    "        while world_state.is_mission_running:\n",
    "\n",
    "            current_r = 0\n",
    "            \n",
    "            if is_first_action:\n",
    "                # wait until have received a valid observation\n",
    "                while True:\n",
    "                    time.sleep(0.1)\n",
    "                    world_state = agent_host.getWorldState()\n",
    "                    for error in world_state.errors:\n",
    "                        self.logger.error(\"Error: %s\" % error.text)\n",
    "                    for reward in world_state.rewards:\n",
    "                        current_r += reward.getValue()\n",
    "                    if world_state.is_mission_running and len(world_state.observations)>0 and not world_state.observations[-1].text==\"{}\":\n",
    "                        total_reward += self.act(world_state, agent_host, current_r)\n",
    "                        break\n",
    "                    if not world_state.is_mission_running:\n",
    "                        break\n",
    "                is_first_action = False\n",
    "            else:\n",
    "                # wait for non-zero reward\n",
    "                while world_state.is_mission_running and current_r == 0:\n",
    "                    time.sleep(0.1)\n",
    "                    world_state = agent_host.getWorldState()\n",
    "                    for error in world_state.errors:\n",
    "                        self.logger.error(\"Error: %s\" % error.text)\n",
    "                    for reward in world_state.rewards:\n",
    "                        current_r += reward.getValue()\n",
    "                # allow time to stabilise after action\n",
    "                while True:\n",
    "                    time.sleep(0.1)\n",
    "                    world_state = agent_host.getWorldState()\n",
    "                    for error in world_state.errors:\n",
    "                        self.logger.error(\"Error: %s\" % error.text)\n",
    "                    for reward in world_state.rewards:\n",
    "                        current_r += reward.getValue()\n",
    "                    if world_state.is_mission_running and len(world_state.observations)>0 and not world_state.observations[-1].text==\"{}\":\n",
    "                        total_reward += self.act(world_state, agent_host, current_r)\n",
    "                        break\n",
    "                    if not world_state.is_mission_running:\n",
    "                        break\n",
    "\n",
    "        # process final reward\n",
    "        self.logger.debug(\"Final reward: %d\" % current_r)\n",
    "        total_reward += current_r\n",
    "\n",
    "        # update Q values\n",
    "        if self.prev_s is not None and self.prev_a is not None:\n",
    "            self.updateQTableFromTerminatingState( current_r )\n",
    "            \n",
    "        self.drawQ()\n",
    "    \n",
    "        return total_reward\n",
    "        \n",
    "    def drawQ( self, curr_x=None, curr_y=None ):\n",
    "        scale = 22\n",
    "        world_x = 35\n",
    "        world_y = 35\n",
    "        if self.canvas is None or self.root is None:\n",
    "            self.root = tk.Tk()\n",
    "            self.root.wm_title(\"Q-table\")\n",
    "            self.canvas = tk.Canvas(self.root, width=world_x*scale, height=world_y*scale, borderwidth=0, highlightthickness=0, bg=\"black\")\n",
    "            self.canvas.grid()\n",
    "            self.root.update()\n",
    "        self.canvas.delete(\"all\")\n",
    "        action_inset = 0.1\n",
    "        action_radius = 0.1\n",
    "        curr_radius = 0.2\n",
    "        action_positions = [ ( 0.5, action_inset ), ( 0.5, 1-action_inset ), ( action_inset, 0.5 ), ( 1-action_inset, 0.5 ) ]\n",
    "        min_value = -20\n",
    "        max_value = 20\n",
    "        for x in range(world_x):\n",
    "            for y in range(world_y):\n",
    "                s = \"%d:%d\" % (x,y)\n",
    "                self.canvas.create_rectangle( x*scale, y*scale, (x+1)*scale, (y+1)*scale, outline=\"#fff\", fill=\"#000\")\n",
    "                for action in range(4):\n",
    "                    if not s in self.q_table:\n",
    "                        continue\n",
    "                    value = self.q_table[s][action]\n",
    "                    color = int( 255 * ( value - min_value ) / ( max_value - min_value ))\n",
    "                    color = max( min( color, 255 ), 0 )\n",
    "                    color_string = '#%02x%02x%02x' % (255-color, color, 0)\n",
    "                    self.canvas.create_oval( (x + action_positions[action][0] - action_radius ) *scale,\n",
    "                                             (y + action_positions[action][1] - action_radius ) *scale,\n",
    "                                             (x + action_positions[action][0] + action_radius ) *scale,\n",
    "                                             (y + action_positions[action][1] + action_radius ) *scale, \n",
    "                                             outline=color_string, fill=color_string )\n",
    "        if curr_x is not None and curr_y is not None:\n",
    "            self.canvas.create_oval( (curr_x + 0.5 - curr_radius ) * scale, \n",
    "                                     (curr_y + 0.5 - curr_radius ) * scale, \n",
    "                                     (curr_x + 0.5 + curr_radius ) * scale, \n",
    "                                     (curr_y + 0.5 + curr_radius ) * scale, \n",
    "                                     outline=\"#fff\", fill=\"#fff\" )\n",
    "        self.root.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR: Caught std::exception: unrecognised option '-f'\n",
      "\n",
      "Malmo version: 0.36.0\n",
      "\n",
      "Allowed options:\n",
      "  -h [ --help ]         show description of allowed options\n",
      "  --test                run this as an integration test\n",
      "\n",
      "\n",
      "Loading mission from ./maze.xml\n",
      "\n",
      "Repeat 1 of 100\n",
      "Waiting for the mission to start ............\n",
      "Mission started \n",
      "--------------------------------\n",
      "Error: AgentHost::sendCommand : commands connection is not open. Is the mission running?\n",
      "Reward obtained:1464\n",
      "Total time trained: 419.32851696014404 seconds\n",
      "Number of actions performed: 537\n",
      "\n",
      "Repeat 2 of 100\n",
      "Waiting for the mission to start .....\n",
      "Mission started \n",
      "--------------------------------\n"
     ]
    }
   ],
   "source": [
    "start_time_all_trials = time.time()\n",
    "results_df = []\n",
    "cumulative_rewards = []\n",
    "\n",
    "agent = GreedyEpsilonQL()\n",
    "agent_host = MalmoPython.AgentHost()\n",
    "\n",
    "try:\n",
    "    agent_host.parse( sys.argv )\n",
    "except RuntimeError as e:\n",
    "    print('ERROR:',e)\n",
    "    print(agent_host.getUsage())\n",
    "    exit(1)\n",
    "if agent_host.receivedArgument(\"help\"):\n",
    "    print(agent_host.getUsage())\n",
    "    exit(0)\n",
    "\n",
    "mission_file = './maze.xml'\n",
    "with open(mission_file, 'r') as f:\n",
    "    print(\"Loading mission from %s\" % mission_file)\n",
    "    mission_xml = f.read()\n",
    "    my_mission = MalmoPython.MissionSpec(mission_xml, True)\n",
    "    \n",
    "agent_host.setObservationsPolicy(MalmoPython.ObservationsPolicy.LATEST_OBSERVATION_ONLY)\n",
    "agent_host.setVideoPolicy(MalmoPython.VideoPolicy.LATEST_FRAME_ONLY)\n",
    "my_mission_record = MalmoPython.MissionRecordSpec()\n",
    "my_mission.requestVideo(800, 500)\n",
    "my_mission.setViewpoint(0)\n",
    "\n",
    "my_clients = MalmoPython.ClientPool()\n",
    "my_clients.add(MalmoPython.ClientInfo('127.0.0.1', 10000)) # add Minecraft machines here as available\n",
    "agentID = 0\n",
    "expID = 'Reward Based Greedy Epsilon Q-Learning.'\n",
    "\n",
    "max_retries = 3\n",
    "\n",
    "if agent_host.receivedArgument(\"test\"):\n",
    "    num_repeats = 1\n",
    "else:\n",
    "    num_repeats = 100\n",
    "\n",
    "for i in range(num_repeats):\n",
    "    \n",
    "    print()\n",
    "    print('Repeat %d of %d' % (i+1, num_repeats ))\n",
    "    \n",
    "    for retry in range(max_retries):\n",
    "        try:\n",
    "            agent_host.startMission(my_mission, my_clients, my_mission_record, agentID, \"%s-%d\" % (expID, i))\n",
    "            break\n",
    "        except RuntimeError as e:\n",
    "            if retry == max_retries - 1:\n",
    "                print(\"Error starting mission:\",e)\n",
    "                exit(1)\n",
    "            else:\n",
    "                time.sleep(2.5)\n",
    "\n",
    "    print(\"Waiting for the mission to start\", end=' ')\n",
    "    world_state = agent_host.getWorldState()\n",
    "    while not world_state.has_mission_begun:\n",
    "        \n",
    "        print(\".\", end=\"\")\n",
    "        time.sleep(0.1)\n",
    "        world_state = agent_host.getWorldState()\n",
    "        for error in world_state.errors:\n",
    "            print()   \n",
    "    print()\n",
    "    \n",
    "    print(\"Mission started \")\n",
    "    print(\"--------------------------------\")\n",
    "    # Run the program.\n",
    "    start_time = time.time()\n",
    "    cumulative_reward = agent.run(agent_host)\n",
    "    print('Reward obtained:%d' % cumulative_reward)\n",
    "    cumulative_rewards += [cumulative_reward]\n",
    "    timeTaken = (time.time() - start_time)\n",
    "    print(\"Total time trained:\", \"%s seconds\" % timeTaken )\n",
    "    print(\"Number of actions performed:\", agent.count_action())\n",
    "    result = [i, cumulative_reward, timeTaken, agent.count_action()]\n",
    "    results_df.append(result)\n",
    "    agent.resetcount_action()\n",
    "\n",
    "print()\n",
    "print(\"Done.\")\n",
    "print(\"Cumulative rewards for all %d runs:\" % num_repeats)\n",
    "print(cumulative_rewards)\n",
    "print(\"Average reward:\", sum(cumulative_rewards)/num_repeats)\n",
    "print(\"Total time used for 100 trials:\", \"%s seconds\" % (time.time() - start_time_all_trials))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "column = ['Number of attempt', 'Reward', 'Time taken', 'Number of actions']\n",
    "results_df = pd.DataFrame(results_df, columns = column)\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Current epsilon value:\", epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.to_csv('GreedyEpsilon_07_M1_F.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
