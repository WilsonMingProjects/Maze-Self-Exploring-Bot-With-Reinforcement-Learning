{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nothing is listening on port 10000 - will attempt to launch Minecraft from a new terminal.\n",
      "Giving Minecraft some time to launch... \n",
      ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ok\n",
      "C:\\Users\\User\\Desktop\\Malmo\\MalmoPlatform\\Schemas\n"
     ]
    }
   ],
   "source": [
    "import malmo.minecraftbootstrap; malmo.minecraftbootstrap.launch_minecraft()\n",
    "malmo.minecraftbootstrap.set_malmo_xsd_path()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from future import standard_library\n",
    "standard_library.install_aliases()\n",
    "from builtins import range\n",
    "import sys\n",
    "import time\n",
    "import json\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Microsoft Malmo API\n",
    "import malmo.MalmoPython as MalmoPython\n",
    "\n",
    "# Check Tkinter version on your pc.\n",
    "if sys.version_info[0] == 2:  \n",
    "    import Tkinter as tk\n",
    "else:\n",
    "    import tkinter as tk\n",
    "\n",
    "from PIL import Image\n",
    "from itertools import count\n",
    "from collections import namedtuple\n",
    "\n",
    "# Neural network API\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "canvas = None\n",
    "root = None\n",
    "q_table = {}\n",
    "Transition = namedtuple('Transition',('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.position = 0\n",
    "\n",
    "    def push(self, *args):\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(None)\n",
    "        self.memory[self.position] = Transition(*args)\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "\n",
    "class DQN(nn.Module):\n",
    "\n",
    "    def __init__(self, h, w, outputs):\n",
    "        super(DQN, self).__init__()\n",
    "        self.conv1 = torch.nn.Conv2d(3, 16, kernel_size=3, stride=1)\n",
    "        self.pool1 = torch.nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.conv2 = torch.nn.Conv2d(16, 8, kernel_size=3, stride=1)\n",
    "        self.pool2 = torch.nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        def conv2d_size_out(size, kernel_size, stride):\n",
    "            return (size - (kernel_size - 1) - 1) // stride  + 1\n",
    "        convw = conv2d_size_out(conv2d_size_out(conv2d_size_out(conv2d_size_out(w,3,1),2,2),3,1),2,2)\n",
    "        convh = conv2d_size_out(conv2d_size_out(conv2d_size_out(conv2d_size_out(h,3,1),2,2),3,1),2,2)\n",
    "        linear_input_size = convw * convh * 8\n",
    "        self.fc1 = torch.nn.Linear(linear_input_size, 64)\n",
    "        self.head = nn.Linear(64, outputs)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool1(x)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool2(x)\n",
    "        x = x.view(-1, 512)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.head(x)\n",
    "        return x\n",
    "\n",
    "def get_screen(world_state):\n",
    "    frame = world_state.video_frames[0] \n",
    "    image = Image.frombytes('RGB', (frame.width, frame.height), bytes(frame.pixels) )\n",
    "    image = np.asarray(image)\n",
    "    screen = image.transpose((2, 0, 1))\n",
    "    screen = np.ascontiguousarray(screen, dtype=np.float32) /255\n",
    "    screen = torch.from_numpy(screen)\n",
    "    resize = T.Compose([T.ToPILImage(),\n",
    "                    T.Resize((40, 40)),\n",
    "                    T.ToTensor()])\n",
    "    return resize(screen).unsqueeze(0)\n",
    "\n",
    "def optimize_model():\n",
    "    #For Batch Calculations\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return\n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "\n",
    "    batch = Transition(*zip(*transitions))\n",
    "\n",
    "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
    "                                          batch.next_state)), dtype=torch.uint8)\n",
    "    non_final_next_states = torch.cat([s for s in batch.next_state\n",
    "                                                if s is not None])\n",
    "    state_batch = torch.cat(batch.state)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "\n",
    "    state_action_values = policy_net(state_batch).gather(1, action_batch)\n",
    "\n",
    "    next_state_values = torch.zeros(BATCH_SIZE)\n",
    "    next_state_values[non_final_mask] = target_net(non_final_next_states).max(1)[0].detach()\n",
    "    # Compute the expected Q values\n",
    "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "    \n",
    "    loss = F.smooth_l1_loss(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "    # Optimize the model\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    for param in policy_net.parameters():\n",
    "        param.grad.data.clamp_(-1, 1)\n",
    "    optimizer.step()\n",
    "    \n",
    "def select_action(state):\n",
    "    global steps_done\n",
    "    sample = random.random()\n",
    "    if steps_done > 764:\n",
    "        eps_threshold = 0\n",
    "    else:\n",
    "        eps_threshold = EPS_END + (EPS_START - EPS_END) * \\\n",
    "        math.exp(-1. * steps_done / EPS_DECAY)\n",
    "    if sample > eps_threshold:\n",
    "        with torch.no_grad():\n",
    "            return policy_net(state).max(1)[1].view(1, 1)\n",
    "    else:\n",
    "        action = torch.tensor([[random.randrange(n_actions)]], dtype=torch.long)\n",
    "        return action\n",
    "\n",
    "def move(A):\n",
    "    #moves given Action A\n",
    "    if A == 0: #forward\n",
    "        agent_host.sendCommand(\"move 1\")\n",
    "    elif A == 1: #backward\n",
    "        agent_host.sendCommand(\"move -1\")\n",
    "    elif A == 2: #left\n",
    "        agent_host.sendCommand(\"turn 1\")\n",
    "    elif A == 3: #right\n",
    "        agent_host.sendCommand(\"turn -1\")\n",
    "\n",
    "def drawQ(curr_x=None, curr_y=None):\n",
    "    global canvas\n",
    "    global root\n",
    "    \n",
    "    scale = 22\n",
    "    world_x = 35\n",
    "    world_y = 35\n",
    "    if canvas is None or root is None:\n",
    "        root = tk.Tk()\n",
    "        root.wm_title(\"Q-table\")\n",
    "        canvas = tk.Canvas(root, width=world_x*scale, height=world_y*scale, borderwidth=0, highlightthickness=0, bg=\"black\")\n",
    "        canvas.grid()\n",
    "        root.update()\n",
    "    canvas.delete(\"all\")\n",
    "    action_inset = 0.1\n",
    "    action_radius = 0.1\n",
    "    curr_radius = 0.2\n",
    "    action_positions = [ ( 0.5, action_inset ), ( 0.5, 1-action_inset ), ( action_inset, 0.5 ), ( 1-action_inset, 0.5 ) ]\n",
    "    min_value = -20\n",
    "    max_value = 20\n",
    "    for x in range(world_x):\n",
    "        for y in range(world_y):\n",
    "            s = \"%d:%d\" % (x,y)\n",
    "            canvas.create_rectangle( x*scale, y*scale, (x+1)*scale, (y+1)*scale, outline=\"#fff\", fill=\"#000\")\n",
    "            for action in range(4):\n",
    "                if not s in q_table:\n",
    "                    continue\n",
    "                value = q_table[s][action]\n",
    "                color = int( 255 * ( value - min_value ) / ( max_value - min_value ))\n",
    "                color = max( min( color, 255 ), 0 )\n",
    "                color_string = '#ff6425'\n",
    "                canvas.create_oval( (x + action_positions[action][0] - action_radius ) *scale,\n",
    "                                         (y + action_positions[action][1] - action_radius ) *scale,\n",
    "                                         (x + action_positions[action][0] + action_radius ) *scale,\n",
    "                                         (y + action_positions[action][1] + action_radius ) *scale, \n",
    "                                         outline=color_string, fill=color_string )\n",
    "    if curr_x is not None and curr_y is not None:\n",
    "        canvas.create_oval( (curr_x + 0.5 - curr_radius ) * scale, \n",
    "                                 (curr_y + 0.5 - curr_radius ) * scale, \n",
    "                                 (curr_x + 0.5 + curr_radius ) * scale, \n",
    "                                 (curr_y + 0.5 + curr_radius ) * scale, \n",
    "                                 outline=\"#fff\", fill=\"#fff\" )\n",
    "    root.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR: Caught std::exception: unrecognised option '-f'\n",
      "\n",
      "Malmo version: 0.36.0\n",
      "\n",
      "Allowed options:\n",
      "  -h [ --help ]         show description of allowed options\n",
      "  --test                run this as an integration test\n",
      "\n",
      "\n",
      "Loading mission from ./maze.xml\n",
      "\n",
      "Repeat 1 of 100\n",
      "Waiting for the mission to start \n",
      "Mission started \n",
      "--------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\anaconda3\\envs\\FYP_Malmo\\lib\\site-packages\\ipykernel_launcher.py:83: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead. (Triggered internally at  ..\\aten\\src\\ATen/native/IndexingUtils.h:25.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward obtained: -5632.0\n",
      "Total time trained: 360.20864510536194 seconds\n",
      "Number of actions performed: 6108\n",
      "\n",
      "Repeat 2 of 100\n",
      "Waiting for the mission to start \n",
      "Mission started \n",
      "--------------------------------\n",
      "Reward obtained: 8589.0\n",
      "Total time trained: 225.29939603805542 seconds\n",
      "Number of actions performed: 3163\n",
      "\n",
      "Repeat 3 of 100\n",
      "Waiting for the mission to start \n",
      "Mission started \n",
      "--------------------------------\n",
      "Reward obtained: -4121.0\n",
      "Total time trained: 360.23427414894104 seconds\n",
      "Number of actions performed: 4865\n",
      "\n",
      "Repeat 4 of 100\n",
      "Waiting for the mission to start \n",
      "Mission started \n",
      "--------------------------------\n",
      "Reward obtained: -4143.0\n",
      "Total time trained: 360.23378896713257 seconds\n",
      "Number of actions performed: 4896\n",
      "\n",
      "Repeat 5 of 100\n",
      "Waiting for the mission to start \n",
      "Mission started \n",
      "--------------------------------\n",
      "Reward obtained: -4198.0\n",
      "Total time trained: 360.2813391685486 seconds\n",
      "Number of actions performed: 4951\n",
      "\n",
      "Repeat 6 of 100\n",
      "Waiting for the mission to start \n",
      "Mission started \n",
      "--------------------------------\n",
      "Reward obtained: 7314.0\n",
      "Total time trained: 338.43531608581543 seconds\n",
      "Number of actions performed: 4480\n",
      "\n",
      "Repeat 7 of 100\n",
      "Waiting for the mission to start \n",
      "Mission started \n",
      "--------------------------------\n",
      "Reward obtained: -5203.0\n",
      "Total time trained: 360.08430099487305 seconds\n",
      "Number of actions performed: 4958\n",
      "\n",
      "Repeat 8 of 100\n",
      "Waiting for the mission to start \n",
      "Mission started \n",
      "--------------------------------\n",
      "Reward obtained: -6825.0\n",
      "Total time trained: 360.09489607810974 seconds\n",
      "Number of actions performed: 6671\n",
      "\n",
      "Repeat 9 of 100\n",
      "Waiting for the mission to start \n",
      "Mission started \n",
      "--------------------------------\n",
      "Reward obtained: -6609.0\n",
      "Total time trained: 360.1312234401703 seconds\n",
      "Number of actions performed: 6879\n",
      "\n",
      "Repeat 10 of 100\n",
      "Waiting for the mission to start \n",
      "Mission started \n",
      "--------------------------------\n",
      "Reward obtained: -5453.0\n",
      "Total time trained: 360.08182430267334 seconds\n",
      "Number of actions performed: 5925\n",
      "\n",
      "Repeat 11 of 100\n",
      "Waiting for the mission to start \n",
      "Mission started \n",
      "--------------------------------\n",
      "Reward obtained: -6920.0\n",
      "Total time trained: 360.1441185474396 seconds\n",
      "Number of actions performed: 7159\n",
      "\n",
      "Repeat 12 of 100\n",
      "Waiting for the mission to start \n",
      "Mission started \n",
      "--------------------------------\n",
      "Reward obtained: 7419.0\n",
      "Total time trained: 242.63986158370972 seconds\n",
      "Number of actions performed: 4845\n",
      "\n",
      "Repeat 13 of 100\n",
      "Waiting for the mission to start \n",
      "Mission started \n",
      "--------------------------------\n",
      "Reward obtained: 9498.0\n",
      "Total time trained: 99.4317376613617 seconds\n",
      "Number of actions performed: 2008\n",
      "\n",
      "Repeat 14 of 100\n",
      "Waiting for the mission to start \n",
      "Mission started \n",
      "--------------------------------\n",
      "Reward obtained: -7538.0\n",
      "Total time trained: 360.16727662086487 seconds\n",
      "Number of actions performed: 7223\n",
      "\n",
      "Repeat 15 of 100\n",
      "Waiting for the mission to start \n",
      "Mission started \n",
      "--------------------------------\n",
      "Reward obtained: -6184.0\n",
      "Total time trained: 360.15543603897095 seconds\n",
      "Number of actions performed: 7392\n",
      "\n",
      "Repeat 16 of 100\n",
      "Waiting for the mission to start \n",
      "Mission started \n",
      "--------------------------------\n",
      "Reward obtained: 1327.0\n",
      "Total time trained: 47.5215539932251 seconds\n",
      "Number of actions performed: 941\n",
      "\n",
      "Repeat 17 of 100\n",
      "Waiting for the mission to start \n",
      "Mission started \n",
      "--------------------------------\n",
      "Reward obtained: 6483.0\n",
      "Total time trained: 195.7944803237915 seconds\n",
      "Number of actions performed: 4012\n",
      "\n",
      "Repeat 18 of 100\n",
      "Waiting for the mission to start \n",
      "Mission started \n",
      "--------------------------------\n",
      "Reward obtained: 8081.0\n",
      "Total time trained: 176.54482412338257 seconds\n",
      "Number of actions performed: 3535\n",
      "\n",
      "Repeat 19 of 100\n",
      "Waiting for the mission to start \n",
      "Mission started \n",
      "--------------------------------\n",
      "Reward obtained: -6911.0\n",
      "Total time trained: 360.15031719207764 seconds\n",
      "Number of actions performed: 7116\n",
      "\n",
      "Repeat 20 of 100\n",
      "Waiting for the mission to start \n",
      "Mission started \n",
      "--------------------------------\n",
      "Reward obtained: -7076.0\n",
      "Total time trained: 360.2275490760803 seconds\n",
      "Number of actions performed: 7310\n",
      "\n",
      "Repeat 21 of 100\n",
      "Waiting for the mission to start \n",
      "Mission started \n",
      "--------------------------------\n",
      "Reward obtained: -7364.0\n",
      "Total time trained: 360.1766674518585 seconds\n",
      "Number of actions performed: 7049\n",
      "\n",
      "Repeat 22 of 100\n",
      "Waiting for the mission to start \n",
      "Mission started \n",
      "--------------------------------\n",
      "Reward obtained: -8452.0\n",
      "Total time trained: 360.17619037628174 seconds\n",
      "Number of actions performed: 7517\n",
      "\n",
      "Repeat 23 of 100\n",
      "Waiting for the mission to start \n",
      "Mission started \n",
      "--------------------------------\n",
      "Reward obtained: -8053.0\n",
      "Total time trained: 360.13212966918945 seconds\n",
      "Number of actions performed: 7612\n",
      "\n",
      "Repeat 24 of 100\n",
      "Waiting for the mission to start \n",
      "Mission started \n",
      "--------------------------------\n",
      "Reward obtained: 8006.0\n",
      "Total time trained: 154.36843585968018 seconds\n",
      "Number of actions performed: 3213\n",
      "\n",
      "Repeat 25 of 100\n",
      "Waiting for the mission to start \n",
      "Mission started \n",
      "--------------------------------\n",
      "Reward obtained: -7528.0\n",
      "Total time trained: 360.1966269016266 seconds\n",
      "Number of actions performed: 7699\n",
      "\n",
      "Repeat 26 of 100\n",
      "Waiting for the mission to start \n",
      "Mission started \n",
      "--------------------------------\n",
      "Reward obtained: 9087.0\n",
      "Total time trained: 92.22089862823486 seconds\n",
      "Number of actions performed: 1968\n",
      "\n",
      "Repeat 27 of 100\n",
      "Waiting for the mission to start \n",
      "Mission started \n",
      "--------------------------------\n",
      "Reward obtained: 8841.0\n",
      "Total time trained: 140.69423174858093 seconds\n",
      "Number of actions performed: 2885\n",
      "\n",
      "Repeat 28 of 100\n",
      "Waiting for the mission to start \n",
      "Mission started \n",
      "--------------------------------\n",
      "Reward obtained: -7878.0\n",
      "Total time trained: 360.2136664390564 seconds\n",
      "Number of actions performed: 7494\n",
      "\n",
      "Repeat 29 of 100\n",
      "Waiting for the mission to start \n",
      "Mission started \n",
      "--------------------------------\n",
      "Reward obtained: -7429.0\n",
      "Total time trained: 360.08811378479004 seconds\n",
      "Number of actions performed: 7178\n",
      "\n",
      "Repeat 30 of 100\n",
      "Waiting for the mission to start \n",
      "Mission started \n",
      "--------------------------------\n",
      "Reward obtained: 9621.0\n",
      "Total time trained: 111.13493466377258 seconds\n",
      "Number of actions performed: 2040\n",
      "\n",
      "Repeat 31 of 100\n",
      "Waiting for the mission to start \n",
      "Mission started \n",
      "--------------------------------\n",
      "Reward obtained: -8490.0\n",
      "Total time trained: 360.1701126098633 seconds\n",
      "Number of actions performed: 7178\n",
      "\n",
      "Repeat 32 of 100\n",
      "Waiting for the mission to start \n",
      "Mission started \n",
      "--------------------------------\n",
      "Reward obtained: 6434.0\n",
      "Total time trained: 269.44211292266846 seconds\n",
      "Number of actions performed: 5534\n",
      "\n",
      "Repeat 33 of 100\n",
      "Waiting for the mission to start \n",
      "Mission started \n",
      "--------------------------------\n",
      "Reward obtained: 6244.0\n",
      "Total time trained: 200.408695936203 seconds\n",
      "Number of actions performed: 4212\n",
      "\n",
      "Repeat 34 of 100\n",
      "Waiting for the mission to start \n",
      "Mission started \n",
      "--------------------------------\n",
      "Reward obtained: -8013.0\n",
      "Total time trained: 360.14584136009216 seconds\n",
      "Number of actions performed: 7601\n",
      "\n",
      "Repeat 35 of 100\n",
      "Waiting for the mission to start \n",
      "Mission started \n",
      "--------------------------------\n",
      "Reward obtained: -7007.0\n",
      "Total time trained: 360.1354727745056 seconds\n",
      "Number of actions performed: 7520\n",
      "\n",
      "Repeat 36 of 100\n",
      "Waiting for the mission to start \n",
      "Mission started \n",
      "--------------------------------\n",
      "Reward obtained: -7042.0\n",
      "Total time trained: 360.1567499637604 seconds\n",
      "Number of actions performed: 7673\n",
      "\n",
      "Repeat 37 of 100\n",
      "Waiting for the mission to start \n",
      "Mission started \n",
      "--------------------------------\n",
      "Reward obtained: -7570.0\n",
      "Total time trained: 360.1870274543762 seconds\n",
      "Number of actions performed: 7622\n",
      "\n",
      "Repeat 38 of 100\n",
      "Waiting for the mission to start \n",
      "Mission started \n",
      "--------------------------------\n",
      "Reward obtained: -7583.0\n",
      "Total time trained: 360.2277398109436 seconds\n",
      "Number of actions performed: 7662\n",
      "\n",
      "Repeat 39 of 100\n",
      "Waiting for the mission to start \n",
      "Mission started \n",
      "--------------------------------\n",
      "Reward obtained: -6340.0\n",
      "Total time trained: 360.21610474586487 seconds\n",
      "Number of actions performed: 7474\n",
      "\n",
      "Repeat 40 of 100\n",
      "Waiting for the mission to start \n",
      "Mission started \n",
      "--------------------------------\n",
      "Reward obtained: 10352.0\n",
      "Total time trained: 73.2304699420929 seconds\n",
      "Number of actions performed: 1252\n",
      "\n",
      "Repeat 41 of 100\n",
      "Waiting for the mission to start \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mission started \n",
      "--------------------------------\n",
      "Reward obtained: -6616.0\n",
      "Total time trained: 360.15013909339905 seconds\n",
      "Number of actions performed: 6092\n",
      "\n",
      "Repeat 42 of 100\n",
      "Waiting for the mission to start \n",
      "Mission started \n",
      "--------------------------------\n",
      "Reward obtained: 4426.0\n",
      "Total time trained: 279.7961513996124 seconds\n",
      "Number of actions performed: 4458\n",
      "\n",
      "Repeat 43 of 100\n",
      "Waiting for the mission to start \n",
      "Mission started \n",
      "--------------------------------\n",
      "Reward obtained: 7565.0\n",
      "Total time trained: 195.09945940971375 seconds\n",
      "Number of actions performed: 3015\n",
      "\n",
      "Repeat 44 of 100\n",
      "Waiting for the mission to start \n",
      "Mission started \n",
      "--------------------------------\n",
      "Reward obtained: 7894.0\n",
      "Total time trained: 180.4364948272705 seconds\n",
      "Number of actions performed: 2830\n",
      "\n",
      "Repeat 45 of 100\n",
      "Waiting for the mission to start \n",
      "Mission started \n",
      "--------------------------------\n",
      "Reward obtained: -6968.0\n",
      "Total time trained: 360.6100687980652 seconds\n",
      "Number of actions performed: 5375\n",
      "\n",
      "Repeat 46 of 100\n",
      "Waiting for the mission to start \n",
      "Mission started \n",
      "--------------------------------\n",
      "Reward obtained: -5957.0\n",
      "Total time trained: 360.26170587539673 seconds\n",
      "Number of actions performed: 5389\n",
      "\n",
      "Repeat 47 of 100\n",
      "Waiting for the mission to start \n",
      "Mission started \n",
      "--------------------------------\n",
      "Reward obtained: 9409.0\n",
      "Total time trained: 94.92516422271729 seconds\n",
      "Number of actions performed: 1293\n",
      "\n",
      "Repeat 48 of 100\n",
      "Waiting for the mission to start \n",
      "Mission started \n",
      "--------------------------------\n",
      "Reward obtained: -6253.0\n",
      "Total time trained: 358.1297216415405 seconds\n",
      "Number of actions performed: 6639\n",
      "\n",
      "Repeat 49 of 100\n",
      "Waiting for the mission to start \n",
      "Mission started \n",
      "--------------------------------\n",
      "Reward obtained: -7366.0\n",
      "Total time trained: 360.05247712135315 seconds\n",
      "Number of actions performed: 7167\n",
      "\n",
      "Repeat 50 of 100\n",
      "Waiting for the mission to start \n",
      "Mission started \n",
      "--------------------------------\n",
      "Reward obtained: -7267.0\n",
      "Total time trained: 360.1736857891083 seconds\n",
      "Number of actions performed: 7419\n",
      "\n",
      "Repeat 51 of 100\n",
      "Waiting for the mission to start \n",
      "Mission started \n",
      "--------------------------------\n",
      "Reward obtained: -6642.0\n",
      "Total time trained: 360.2063338756561 seconds\n",
      "Number of actions performed: 7268\n",
      "\n",
      "Repeat 52 of 100\n",
      "Waiting for the mission to start \n",
      "Mission started \n",
      "--------------------------------\n",
      "Reward obtained: -1193.0\n",
      "Total time trained: 104.72342109680176 seconds\n",
      "Number of actions performed: 2146\n",
      "\n",
      "Repeat 53 of 100\n",
      "Waiting for the mission to start \n",
      "Mission started \n",
      "--------------------------------\n",
      "Reward obtained: -1487.0\n",
      "Total time trained: 152.31189966201782 seconds\n",
      "Number of actions performed: 3198\n",
      "\n",
      "Repeat 54 of 100\n",
      "Waiting for the mission to start \n",
      "Mission started \n",
      "--------------------------------\n",
      "Reward obtained: -7040.0\n",
      "Total time trained: 360.1940107345581 seconds\n",
      "Number of actions performed: 7610\n",
      "\n",
      "Repeat 55 of 100\n",
      "Waiting for the mission to start \n",
      "Mission started \n",
      "--------------------------------\n",
      "Reward obtained: -6830.0\n",
      "Total time trained: 352.5790901184082 seconds\n",
      "Number of actions performed: 7572\n",
      "\n",
      "Repeat 56 of 100\n",
      "Waiting for the mission to start \n",
      "Mission started \n",
      "--------------------------------\n",
      "Reward obtained: -7563.0\n",
      "Total time trained: 360.1934885978699 seconds\n",
      "Number of actions performed: 7637\n",
      "\n",
      "Repeat 57 of 100\n",
      "Waiting for the mission to start \n",
      "Mission started \n",
      "--------------------------------\n",
      "Reward obtained: -8096.0\n",
      "Total time trained: 360.3146071434021 seconds\n",
      "Number of actions performed: 7676\n",
      "\n",
      "Repeat 58 of 100\n",
      "Waiting for the mission to start \n",
      "Mission started \n",
      "--------------------------------\n",
      "Reward obtained: 8761.0\n",
      "Total time trained: 86.57955932617188 seconds\n",
      "Number of actions performed: 1783\n",
      "\n",
      "Repeat 59 of 100\n",
      "Waiting for the mission to start \n",
      "Mission started \n",
      "--------------------------------\n",
      "Reward obtained: -7663.0\n",
      "Total time trained: 360.1650114059448 seconds\n",
      "Number of actions performed: 7719\n",
      "\n",
      "Repeat 60 of 100\n",
      "Waiting for the mission to start \n",
      "Mission started \n",
      "--------------------------------\n",
      "Reward obtained: -7572.0\n",
      "Total time trained: 360.04227352142334 seconds\n",
      "Number of actions performed: 7658\n",
      "\n",
      "Repeat 61 of 100\n",
      "Waiting for the mission to start \n",
      "Mission started \n",
      "--------------------------------\n",
      "Reward obtained: 8343.0\n",
      "Total time trained: 119.8729944229126 seconds\n",
      "Number of actions performed: 2504\n",
      "\n",
      "Repeat 62 of 100\n",
      "Waiting for the mission to start \n",
      "Mission started \n",
      "--------------------------------\n",
      "Reward obtained: 9309.0\n",
      "Total time trained: 120.01393151283264 seconds\n",
      "Number of actions performed: 2559\n",
      "\n",
      "Repeat 63 of 100\n",
      "Waiting for the mission to start \n",
      "Mission started \n",
      "--------------------------------\n",
      "Reward obtained: 4271.0\n",
      "Total time trained: 324.87422609329224 seconds\n",
      "Number of actions performed: 6974\n",
      "\n",
      "Repeat 64 of 100\n",
      "Waiting for the mission to start \n",
      "Mission started \n",
      "--------------------------------\n",
      "Reward obtained: -7154.0\n",
      "Total time trained: 360.07415533065796 seconds\n",
      "Number of actions performed: 6910\n",
      "\n",
      "Repeat 65 of 100\n",
      "Waiting for the mission to start \n",
      "Mission started \n",
      "--------------------------------\n",
      "Reward obtained: -5537.0\n",
      "Total time trained: 360.1634066104889 seconds\n",
      "Number of actions performed: 5646\n",
      "\n",
      "Repeat 66 of 100\n",
      "Waiting for the mission to start \n",
      "Mission started \n",
      "--------------------------------\n",
      "Reward obtained: -5862.0\n",
      "Total time trained: 371.9570109844208 seconds\n",
      "Number of actions performed: 5924\n",
      "\n",
      "Repeat 67 of 100\n",
      "Waiting for the mission to start \n",
      "Mission started \n",
      "--------------------------------\n",
      "Reward obtained: -4663.0\n",
      "Total time trained: 410.3859658241272 seconds\n",
      "Number of actions performed: 6087\n",
      "\n",
      "Repeat 68 of 100\n",
      "Waiting for the mission to start \n",
      "Mission started \n",
      "--------------------------------\n",
      "Reward obtained: -6189.0\n",
      "Total time trained: 388.58456206321716 seconds\n",
      "Number of actions performed: 6959\n",
      "\n",
      "Repeat 69 of 100\n",
      "Waiting for the mission to start \n",
      "Mission started \n",
      "--------------------------------\n",
      "Reward obtained: -6067.0\n",
      "Total time trained: 439.6574082374573 seconds\n",
      "Number of actions performed: 8017\n",
      "\n",
      "Repeat 70 of 100\n",
      "Waiting for the mission to start \n",
      "Mission started \n",
      "--------------------------------\n",
      "Reward obtained: 10626.0\n",
      "Total time trained: 114.72626614570618 seconds\n",
      "Number of actions performed: 2267\n",
      "\n",
      "Repeat 71 of 100\n",
      "Waiting for the mission to start \n",
      "Mission started \n",
      "--------------------------------\n",
      "Reward obtained: -7570.0\n",
      "Total time trained: 378.61215329170227 seconds\n",
      "Number of actions performed: 7672\n",
      "\n",
      "Repeat 72 of 100\n",
      "Waiting for the mission to start \n",
      "Mission started \n",
      "--------------------------------\n",
      "Reward obtained: 7928.0\n",
      "Total time trained: 223.77351760864258 seconds\n",
      "Number of actions performed: 4484\n",
      "\n",
      "Repeat 73 of 100\n",
      "Waiting for the mission to start \n",
      "Mission started \n",
      "--------------------------------\n",
      "Reward obtained: -6846.0\n",
      "Total time trained: 360.1110084056854 seconds\n",
      "Number of actions performed: 7489\n",
      "\n",
      "Repeat 74 of 100\n",
      "Waiting for the mission to start \n",
      "Mission started \n",
      "--------------------------------\n",
      "Reward obtained: -7199.0\n",
      "Total time trained: 360.08871579170227 seconds\n",
      "Number of actions performed: 7337\n",
      "\n",
      "Repeat 75 of 100\n",
      "Waiting for the mission to start \n",
      "Mission started \n",
      "--------------------------------\n",
      "Reward obtained: -7401.0\n",
      "Total time trained: 360.1122992038727 seconds\n",
      "Number of actions performed: 7449\n",
      "\n",
      "Repeat 76 of 100\n",
      "Waiting for the mission to start \n",
      "Mission started \n",
      "--------------------------------\n",
      "Reward obtained: 6809.0\n",
      "Total time trained: 203.94819593429565 seconds\n",
      "Number of actions performed: 4157\n",
      "\n",
      "Repeat 77 of 100\n",
      "Waiting for the mission to start \n",
      "Mission started \n",
      "--------------------------------\n",
      "Reward obtained: -7087.0\n",
      "Total time trained: 360.1617867946625 seconds\n",
      "Number of actions performed: 6657\n",
      "\n",
      "Repeat 78 of 100\n",
      "Waiting for the mission to start \n",
      "Mission started \n",
      "--------------------------------\n",
      "Reward obtained: -9002.0\n",
      "Total time trained: 360.1944274902344 seconds\n",
      "Number of actions performed: 6604\n",
      "\n",
      "Repeat 79 of 100\n",
      "Waiting for the mission to start \n",
      "Mission started \n",
      "--------------------------------\n",
      "Reward obtained: -7069.0\n",
      "Total time trained: 360.0747621059418 seconds\n",
      "Number of actions performed: 6334\n",
      "\n",
      "Repeat 80 of 100\n",
      "Waiting for the mission to start \n",
      "Mission started \n",
      "--------------------------------\n",
      "Reward obtained: -7179.0\n",
      "Total time trained: 360.12671089172363 seconds\n",
      "Number of actions performed: 6559\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repeat 81 of 100\n",
      "Waiting for the mission to start \n",
      "Mission started \n",
      "--------------------------------\n",
      "Reward obtained: 8361.0\n",
      "Total time trained: 155.81680560112 seconds\n",
      "Number of actions performed: 2910\n",
      "\n",
      "Repeat 82 of 100\n",
      "Waiting for the mission to start \n",
      "Mission started \n",
      "--------------------------------\n",
      "Reward obtained: -7556.0\n",
      "Total time trained: 360.12913060188293 seconds\n",
      "Number of actions performed: 6541\n",
      "\n",
      "Repeat 83 of 100\n",
      "Waiting for the mission to start \n",
      "Mission started \n",
      "--------------------------------\n",
      "Reward obtained: -5605.0\n",
      "Total time trained: 360.111492395401 seconds\n",
      "Number of actions performed: 6655\n",
      "\n",
      "Repeat 84 of 100\n",
      "Waiting for the mission to start \n",
      "Mission started \n",
      "--------------------------------\n",
      "Reward obtained: 9171.0\n",
      "Total time trained: 90.18959259986877 seconds\n",
      "Number of actions performed: 1503\n",
      "\n",
      "Repeat 85 of 100\n",
      "Waiting for the mission to start \n",
      "Mission started \n",
      "--------------------------------\n",
      "Reward obtained: 6204.0\n",
      "Total time trained: 198.63978624343872 seconds\n",
      "Number of actions performed: 3697\n",
      "\n",
      "Repeat 86 of 100\n",
      "Waiting for the mission to start \n",
      "Mission started \n",
      "--------------------------------\n",
      "Reward obtained: -7152.0\n",
      "Total time trained: 360.19099283218384 seconds\n",
      "Number of actions performed: 6669\n",
      "\n",
      "Repeat 87 of 100\n",
      "Waiting for the mission to start \n",
      "Mission started \n",
      "--------------------------------\n",
      "Reward obtained: -7491.0\n",
      "Total time trained: 360.15400528907776 seconds\n",
      "Number of actions performed: 6755\n",
      "\n",
      "Repeat 88 of 100\n",
      "Waiting for the mission to start \n",
      "Mission started \n",
      "--------------------------------\n",
      "Reward obtained: -7638.0\n",
      "Total time trained: 360.21688652038574 seconds\n",
      "Number of actions performed: 6159\n",
      "\n",
      "Repeat 89 of 100\n",
      "Waiting for the mission to start \n",
      "Mission started \n",
      "--------------------------------\n",
      "Reward obtained: 8847.0\n",
      "Total time trained: 119.78906273841858 seconds\n",
      "Number of actions performed: 1940\n",
      "\n",
      "Repeat 90 of 100\n",
      "Waiting for the mission to start \n",
      "Mission started \n",
      "--------------------------------\n",
      "Reward obtained: -6950.0\n",
      "Total time trained: 360.0620620250702 seconds\n",
      "Number of actions performed: 5913\n",
      "\n",
      "Repeat 91 of 100\n",
      "Waiting for the mission to start \n",
      "Mission started \n",
      "--------------------------------\n",
      "Reward obtained: -8024.0\n",
      "Total time trained: 360.2685992717743 seconds\n",
      "Number of actions performed: 5861\n",
      "\n",
      "Repeat 92 of 100\n",
      "Waiting for the mission to start \n",
      "Mission started \n",
      "--------------------------------\n",
      "Reward obtained: -7090.0\n",
      "Total time trained: 360.21287655830383 seconds\n",
      "Number of actions performed: 6006\n",
      "\n",
      "Repeat 93 of 100\n",
      "Waiting for the mission to start \n",
      "Mission started \n",
      "--------------------------------\n",
      "Reward obtained: 2658.0\n",
      "Total time trained: 354.8371057510376 seconds\n",
      "Number of actions performed: 5777\n",
      "\n",
      "Repeat 94 of 100\n",
      "Waiting for the mission to start \n",
      "Mission started \n",
      "--------------------------------\n",
      "Reward obtained: 6707.0\n",
      "Total time trained: 218.7527096271515 seconds\n",
      "Number of actions performed: 3527\n",
      "\n",
      "Repeat 95 of 100\n",
      "Waiting for the mission to start \n",
      "Mission started \n",
      "--------------------------------\n",
      "Reward obtained: -5985.0\n",
      "Total time trained: 360.1486876010895 seconds\n",
      "Number of actions performed: 6014\n",
      "\n",
      "Repeat 96 of 100\n",
      "Waiting for the mission to start \n",
      "Mission started \n",
      "--------------------------------\n",
      "Reward obtained: -4917.0\n",
      "Total time trained: 204.30150651931763 seconds\n",
      "Number of actions performed: 3635\n",
      "\n",
      "Repeat 97 of 100\n",
      "Waiting for the mission to start \n",
      "Mission started \n",
      "--------------------------------\n",
      "Reward obtained: 9556.0\n",
      "Total time trained: 93.86072874069214 seconds\n",
      "Number of actions performed: 1777\n",
      "\n",
      "Repeat 98 of 100\n",
      "Waiting for the mission to start \n",
      "Mission started \n",
      "--------------------------------\n",
      "Reward obtained: -7293.0\n",
      "Total time trained: 360.1756737232208 seconds\n",
      "Number of actions performed: 7253\n",
      "\n",
      "Repeat 99 of 100\n",
      "Waiting for the mission to start \n",
      "Mission started \n",
      "--------------------------------\n",
      "Reward obtained: -7460.0\n",
      "Total time trained: 360.06929111480713 seconds\n",
      "Number of actions performed: 7463\n",
      "\n",
      "Repeat 100 of 100\n",
      "Waiting for the mission to start \n",
      "Mission started \n",
      "--------------------------------\n",
      "Reward obtained: -2326.0\n",
      "Total time trained: 126.63115739822388 seconds\n",
      "Number of actions performed: 2644\n",
      "\n",
      "Done\n",
      "Cumulative rewards for all 100 runs:\n",
      "[-5632.0, 8589.0, -4121.0, -4143.0, -4198.0, 7314.0, -5203.0, -6825.0, -6609.0, -5453.0, -6920.0, 7419.0, 9498.0, -7538.0, -6184.0, 1327.0, 6483.0, 8081.0, -6911.0, -7076.0, -7364.0, -8452.0, -8053.0, 8006.0, -7528.0, 9087.0, 8841.0, -7878.0, -7429.0, 9621.0, -8490.0, 6434.0, 6244.0, -8013.0, -7007.0, -7042.0, -7570.0, -7583.0, -6340.0, 10352.0, -6616.0, 4426.0, 7565.0, 7894.0, -6968.0, -5957.0, 9409.0, -6253.0, -7366.0, -7267.0, -6642.0, -1193.0, -1487.0, -7040.0, -6830.0, -7563.0, -8096.0, 8761.0, -7663.0, -7572.0, 8343.0, 9309.0, 4271.0, -7154.0, -5537.0, -5862.0, -4663.0, -6189.0, -6067.0, 10626.0, -7570.0, 7928.0, -6846.0, -7199.0, -7401.0, 6809.0, -7087.0, -9002.0, -7069.0, -7179.0, 8361.0, -7556.0, -5605.0, 9171.0, 6204.0, -7152.0, -7491.0, -7638.0, 8847.0, -6950.0, -8024.0, -7090.0, 2658.0, 6707.0, -5985.0, -4917.0, 9556.0, -7293.0, -7460.0, -2326.0]\n",
      "Average reward: -2062.46\n",
      "Total time used for 30 trials: 29433.57649374008 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time_all_trials = time.time()\n",
    "results_df = []\n",
    "cumulative_rewards = []\n",
    "\n",
    "# DQN vairables\n",
    "BATCH_SIZE = 64\n",
    "GAMMA = 0.1\n",
    "EPS_START = 0.8\n",
    "EPS_END = 0.1\n",
    "EPS_DECAY = 45\n",
    "TARGET_UPDATE = 10\n",
    "n_actions = 4\n",
    "lr = 0.001\n",
    "\n",
    "screen_height = 40 \n",
    "screen_width = 40\n",
    "policy_net = DQN(screen_height, screen_width, n_actions)\n",
    "target_net = DQN(screen_height, screen_width, n_actions)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval()\n",
    "optimizer = optim.Adam(params=policy_net.parameters(), lr=lr)\n",
    "memory = ReplayMemory(1000)\n",
    "\n",
    "agent_host = MalmoPython.AgentHost()\n",
    "\n",
    "try:\n",
    "    agent_host.parse( sys.argv )\n",
    "except RuntimeError as e:\n",
    "    print('ERROR:',e)\n",
    "    print(agent_host.getUsage())\n",
    "    exit(1)\n",
    "if agent_host.receivedArgument(\"help\"):\n",
    "    print(agent_host.getUsage())\n",
    "    exit(0)\n",
    "\n",
    "mission_file = './maze.xml'\n",
    "with open(mission_file, 'r') as f:\n",
    "    print(\"Loading mission from %s\" % mission_file)\n",
    "    mission_xml = f.read()\n",
    "    my_mission = MalmoPython.MissionSpec(mission_xml, True)\n",
    "    \n",
    "print()\n",
    "agent_host.setObservationsPolicy(MalmoPython.ObservationsPolicy.LATEST_OBSERVATION_ONLY)\n",
    "agent_host.setVideoPolicy(MalmoPython.VideoPolicy.LATEST_FRAME_ONLY)\n",
    "my_mission_record = MalmoPython.MissionRecordSpec()\n",
    "my_mission.requestVideo(800, 500)\n",
    "my_mission.setViewpoint(0)\n",
    "\n",
    "my_clients = MalmoPython.ClientPool()\n",
    "my_clients.add(MalmoPython.ClientInfo('127.0.0.1', 10000)) # add Minecraft machines here as available\n",
    "agentID = 0\n",
    "expID = 'Deep Q Network'\n",
    "\n",
    "max_retries = 3\n",
    "\n",
    "if agent_host.receivedArgument(\"test\"):\n",
    "    num_repeats = 1\n",
    "else:\n",
    "    num_repeats = 100\n",
    "\n",
    "for i in range(num_repeats): #loop for training runs \n",
    "    \n",
    "    action_count = 0\n",
    "    steps_done = 0\n",
    "    \n",
    "    for retry in range(max_retries):\n",
    "        try:\n",
    "            agent_host.startMission( my_mission, my_clients, my_mission_record, agentID, \"%s-%d\" % (expID, i)) #replace to start on a different client\n",
    "            break\n",
    "        except RuntimeError as e:\n",
    "            if retry == max_retries - 1:\n",
    "                print(\"Error starting mission:\",e)\n",
    "                exit(1)\n",
    "            else:\n",
    "                time.sleep(2)\n",
    "        \n",
    "    # Loop until mission starts:\n",
    "    print('Repeat %d of %d' % (i+1, num_repeats ))\n",
    "    print(\"Waiting for the mission to start \")\n",
    "    world_state = agent_host.getWorldState()\n",
    "    while not world_state.has_mission_begun:\n",
    "        time.sleep(0.1)\n",
    "        world_state = agent_host.getWorldState()\n",
    "        for error in world_state.errors:\n",
    "            print()\n",
    "\n",
    "    # wait for a valid observation\n",
    "    world_state = agent_host.peekWorldState()\n",
    "    while world_state.is_mission_running and all(e.text=='{}' for e in world_state.observations):\n",
    "        world_state = agent_host.peekWorldState()\n",
    "    # wait for a frame to arrive after that\n",
    "    num_frames_seen = world_state.number_of_video_frames_since_last_state\n",
    "    while world_state.is_mission_running and world_state.number_of_video_frames_since_last_state == num_frames_seen:\n",
    "        world_state = agent_host.peekWorldState()\n",
    "    world_state = agent_host.getWorldState()\n",
    "    for error in world_state.errors:\n",
    "        print(error)\n",
    "\n",
    "    state = get_screen(world_state)\n",
    "    R_total = 0 #Sum of Rewards\n",
    "    framecount=0\n",
    "    \n",
    "    start_time = time.time()\n",
    "    print(\"Mission started \")\n",
    "    print(\"--------------------------------\")\n",
    "    while world_state.is_mission_running:\n",
    "\n",
    "        action = select_action(state)\n",
    "        move(action.item())\n",
    "        \n",
    "        # Count action performed\n",
    "        if(action.item()):\n",
    "            action_count += 1\n",
    "\n",
    "        world_state = agent_host.peekWorldState()\n",
    "        while world_state.number_of_video_frames_since_last_state < 1 and world_state.is_mission_running:\n",
    "            world_state = agent_host.peekWorldState()                \n",
    "        world_state = agent_host.getWorldState()\n",
    "\n",
    "        if world_state.is_mission_running:\n",
    "            next_state = get_screen(world_state)\n",
    "\n",
    "        elif not world_state.is_mission_running:\n",
    "            next_state = None\n",
    "\n",
    "        reward = sum(r.getValue() for r in world_state.rewards)\n",
    "        R_total += reward\n",
    "        reward = torch.tensor([reward], dtype=torch.float)\n",
    "\n",
    "        memory.push(state, action, next_state, reward)\n",
    "\n",
    "        # Move to the next state\n",
    "        state = next_state\n",
    "        \n",
    "        if world_state.is_mission_running and len(world_state.observations)>0 and not world_state.observations[-1].text==\"{}\":\n",
    "            obs_text = world_state.observations[-1].text\n",
    "            obs = json.loads(obs_text) # most recent observation\n",
    "            #print(float(obs[u'XPos']),\":\", float(obs[u'ZPos']))\n",
    "            #drawQ(curr_x = int(obs[u'XPos']), curr_y = int(obs[u'ZPos']))\n",
    "            \n",
    "    optimize_model()\n",
    "\n",
    "    # Update the target network, copying all weights and biases in DQN\n",
    "    if i % TARGET_UPDATE == 0:\n",
    "        target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "    steps_done += 1  \n",
    "    \n",
    "    print(\"Reward obtained:\", R_total)  \n",
    "    cumulative_rewards += [R_total]\n",
    "    timeTaken = (time.time() - start_time)\n",
    "    print(\"Total time trained:\", \"%s seconds\" % timeTaken )\n",
    "    print(\"Number of actions performed:\", action_count)\n",
    "    result = [i, R_total, timeTaken, action_count]\n",
    "    results_df.append(result)\n",
    "    print()\n",
    "    # Mission has ended.\n",
    "\n",
    "print('Done')\n",
    "print(\"Cumulative rewards for all %d runs:\" % num_repeats)\n",
    "print(cumulative_rewards)\n",
    "print(\"Average reward:\", sum(cumulative_rewards)/num_repeats)\n",
    "print(\"Total time used for 30 trials:\", \"%s seconds\" % (time.time() - start_time_all_trials))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Number of attempt</th>\n",
       "      <th>Reward</th>\n",
       "      <th>Time taken</th>\n",
       "      <th>Number of actions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>-5632.0</td>\n",
       "      <td>360.208645</td>\n",
       "      <td>6108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>8589.0</td>\n",
       "      <td>225.299396</td>\n",
       "      <td>3163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>-4121.0</td>\n",
       "      <td>360.234274</td>\n",
       "      <td>4865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>-4143.0</td>\n",
       "      <td>360.233789</td>\n",
       "      <td>4896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>-4198.0</td>\n",
       "      <td>360.281339</td>\n",
       "      <td>4951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>95</td>\n",
       "      <td>-4917.0</td>\n",
       "      <td>204.301507</td>\n",
       "      <td>3635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>96</td>\n",
       "      <td>9556.0</td>\n",
       "      <td>93.860729</td>\n",
       "      <td>1777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>97</td>\n",
       "      <td>-7293.0</td>\n",
       "      <td>360.175674</td>\n",
       "      <td>7253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>98</td>\n",
       "      <td>-7460.0</td>\n",
       "      <td>360.069291</td>\n",
       "      <td>7463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>99</td>\n",
       "      <td>-2326.0</td>\n",
       "      <td>126.631157</td>\n",
       "      <td>2644</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows  4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Number of attempt  Reward  Time taken  Number of actions\n",
       "0                   0 -5632.0  360.208645               6108\n",
       "1                   1  8589.0  225.299396               3163\n",
       "2                   2 -4121.0  360.234274               4865\n",
       "3                   3 -4143.0  360.233789               4896\n",
       "4                   4 -4198.0  360.281339               4951\n",
       "..                ...     ...         ...                ...\n",
       "95                 95 -4917.0  204.301507               3635\n",
       "96                 96  9556.0   93.860729               1777\n",
       "97                 97 -7293.0  360.175674               7253\n",
       "98                 98 -7460.0  360.069291               7463\n",
       "99                 99 -2326.0  126.631157               2644\n",
       "\n",
       "[100 rows x 4 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "column = ['Number of attempt', 'Reward', 'Time taken', 'Number of actions']\n",
    "results_df = pd.DataFrame(results_df, columns = column)\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.to_csv('DQN_E7_M1.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
