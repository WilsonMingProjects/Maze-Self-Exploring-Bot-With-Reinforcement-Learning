{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nothing is listening on port 10000 - will attempt to launch Minecraft from a new terminal.\n",
      "Giving Minecraft some time to launch... \n",
      ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ok\n",
      "C:\\Users\\User\\Desktop\\Malmo\\MalmoPlatform\\Schemas\n"
     ]
    }
   ],
   "source": [
    "import malmo.minecraftbootstrap; malmo.minecraftbootstrap.launch_minecraft()\n",
    "malmo.minecraftbootstrap.set_malmo_xsd_path()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from future import standard_library\n",
    "standard_library.install_aliases()\n",
    "from builtins import range\n",
    "from builtins import object\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import json\n",
    "import random\n",
    "import logging\n",
    "\n",
    "# Microsoft Malmo API\n",
    "import malmo.MalmoPython as MalmoPython\n",
    "\n",
    "# Check Tkinter version on your pc.\n",
    "if sys.version_info[0] == 2:  \n",
    "    import Tkinter as tk\n",
    "else:\n",
    "    import tkinter as tk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define agent's action\n",
    "agent_actions = [\"move 1\", \"move -1\", \"turn 1\", \"turn -1\"]\n",
    "\n",
    "# Appoint value to alpha, epsilon, and gamma.\n",
    "epsilon = 0.6\n",
    "alpha = 0.1\n",
    "gamma = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QL_AEG(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.epsilon = epsilon\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.training = True\n",
    "        \n",
    "        # Initialize the action count\n",
    "        self.count = 0\n",
    "\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "        if False:\n",
    "            self.logger.setLevel(logging.DEBUG)\n",
    "        else:\n",
    "            self.logger.setLevel(logging.INFO)\n",
    "        self.logger.handlers = []\n",
    "        self.logger.addHandler(logging.StreamHandler(sys.stdout))\n",
    "\n",
    "        self.actions = agent_actions\n",
    "        self.q_table = {}\n",
    "        self.canvas = None\n",
    "        self.root = None\n",
    "        \n",
    "    def count_action(self):\n",
    "        return self.count\n",
    "    \n",
    "    def resetcount_action(self):\n",
    "        self.count = 0\n",
    "        return self.count\n",
    "    \n",
    "    def loadModel(self, model_file):\n",
    "        with open(model_file) as f:\n",
    "            self.q_table = json.load(f)\n",
    "            \n",
    "    def training(self):\n",
    "        self.training = True\n",
    "        \n",
    "    def evaluate(self):\n",
    "        self.training = False\n",
    "        \n",
    "    def act(self, world_state, agent_host, current_r ):\n",
    "        \n",
    "        obs_text = world_state.observations[-1].text\n",
    "        obs = json.loads(obs_text) # most recent observation\n",
    "        self.logger.debug(obs)\n",
    "        if not u'XPos' in obs or not u'ZPos' in obs:\n",
    "            self.logger.error(\"Incomplete observation received: %s\" % obs_text)\n",
    "            return 0\n",
    "        current_s = \"%d:%d\" % (int(obs[u'XPos']), int(obs[u'ZPos']))\n",
    "        self.logger.debug(\"State: %s (x = %.2f, z = %.2f)\" % (current_s, float(obs[u'XPos']), float(obs[u'ZPos'])))\n",
    "        if current_s not in self.q_table:\n",
    "            self.q_table[current_s] = ([0] * len(self.actions))\n",
    "\n",
    "        # update Q values\n",
    "        if self.training and self.prev_s is not None and self.prev_a is not None:\n",
    "            old_q = self.q_table[self.prev_s][self.prev_a]\n",
    "            self.q_table[self.prev_s][self.prev_a] = old_q + self.alpha * (current_r\n",
    "                + self.gamma * max(self.q_table[current_s]) - old_q)\n",
    "\n",
    "        self.drawQ( curr_x = int(obs[u'XPos']), curr_y = int(obs[u'ZPos']) )\n",
    "\n",
    "        # select the next action\n",
    "        rnd = random.random()\n",
    "        if rnd < self.epsilon:\n",
    "            a = random.randint(0, len(self.actions) - 1)\n",
    "            self.actions[a]\n",
    "        else:\n",
    "            m = max(self.q_table[current_s])\n",
    "            self.logger.debug(\"Current values: %s\" % \",\".join(str(x) for x in self.q_table[current_s]))\n",
    "            l = list()\n",
    "            for x in range(0, len(self.actions)):\n",
    "                if self.q_table[current_s][x] == m:\n",
    "                    l.append(x)\n",
    "            y = random.randint(0, len(l)-1)\n",
    "            a = l[y]\n",
    "            self.actions[a]\n",
    "        \n",
    "        # try to send the selected action, only update prev_s if this succeeds\n",
    "        try:\n",
    "            agent_host.sendCommand(self.actions[a])\n",
    "            self.prev_s = current_s\n",
    "            self.prev_a = a\n",
    "\n",
    "        except RuntimeError as e:\n",
    "            self.logger.error(\"Failed to send command: %s\" % e)\n",
    "        \n",
    "        self.count += 1\n",
    "        return current_r\n",
    "\n",
    "    def run(self, agent_host):\n",
    "        total_reward = 0   \n",
    "        self.prev_s = None\n",
    "        self.prev_a = None  \n",
    "        is_first_action = True\n",
    "        \n",
    "        # main loop:\n",
    "        world_state = agent_host.getWorldState()\n",
    "        while world_state.is_mission_running:\n",
    "\n",
    "            current_r = 0       \n",
    "            if is_first_action:\n",
    "                # wait until have received a valid observation\n",
    "                while True:\n",
    "                    time.sleep(0.1)\n",
    "                    world_state = agent_host.getWorldState()\n",
    "                    for error in world_state.errors:\n",
    "                        self.logger.error(\"Error: %s\" % error.text)\n",
    "                    for reward in world_state.rewards:\n",
    "                        current_r += reward.getValue()\n",
    "                    if world_state.is_mission_running and len(world_state.observations)>0 and not world_state.observations[-1].text==\"{}\":\n",
    "                        total_reward += self.act(world_state, agent_host, current_r)\n",
    "                        break\n",
    "                    if not world_state.is_mission_running:\n",
    "                        break\n",
    "                is_first_action = False\n",
    "            else:\n",
    "                # wait for non-zero reward\n",
    "                while world_state.is_mission_running and current_r == 0:\n",
    "                    time.sleep(0.1)\n",
    "                    world_state = agent_host.getWorldState()\n",
    "                    for error in world_state.errors:\n",
    "                        self.logger.error(\"Error: %s\" % error.text)\n",
    "                    for reward in world_state.rewards:\n",
    "                        current_r += reward.getValue()\n",
    "                # allow time to stabilise after action\n",
    "                while True:\n",
    "                    time.sleep(0.1)\n",
    "                    world_state = agent_host.getWorldState()\n",
    "                    for error in world_state.errors:\n",
    "                        self.logger.error(\"Error: %s\" % error.text)\n",
    "                    for reward in world_state.rewards:\n",
    "                        current_r += reward.getValue()\n",
    "                    if world_state.is_mission_running and len(world_state.observations)>0 and not world_state.observations[-1].text==\"{}\":\n",
    "                        total_reward += self.act(world_state, agent_host, current_r)\n",
    "                        break\n",
    "                    if not world_state.is_mission_running:\n",
    "                        break\n",
    "\n",
    "        # process final reward\n",
    "        self.logger.debug(\"Final reward: %d\" % current_r)\n",
    "        total_reward += current_r\n",
    "\n",
    "        # update Q values\n",
    "        if self.training and self.prev_s is not None and self.prev_a is not None:\n",
    "            old_q = self.q_table[self.prev_s][self.prev_a]\n",
    "            self.q_table[self.prev_s][self.prev_a] = old_q + self.alpha * (current_r - old_q)\n",
    "            \n",
    "        self.drawQ()\n",
    "    \n",
    "        return total_reward\n",
    "        \n",
    "    def drawQ( self, curr_x=None, curr_y=None ):\n",
    "        scale = 22\n",
    "        world_x = 35\n",
    "        world_y = 35\n",
    "        if self.canvas is None or self.root is None:\n",
    "            self.root = tk.Tk()\n",
    "            self.root.wm_title(\"Q-table\")\n",
    "            self.canvas = tk.Canvas(self.root, width=world_x*scale, height=world_y*scale, borderwidth=0, highlightthickness=0, bg=\"black\")\n",
    "            self.canvas.grid()\n",
    "            self.root.update()\n",
    "        self.canvas.delete(\"all\")\n",
    "        action_inset = 0.1\n",
    "        action_radius = 0.1\n",
    "        curr_radius = 0.2\n",
    "        action_positions = [ ( 0.5, action_inset ), ( 0.5, 1-action_inset ), ( action_inset, 0.5 ), ( 1-action_inset, 0.5 ) ]\n",
    "        min_value = -20\n",
    "        max_value = 20\n",
    "        for x in range(world_x):\n",
    "            for y in range(world_y):\n",
    "                s = \"%d:%d\" % (x,y)\n",
    "                self.canvas.create_rectangle( x*scale, y*scale, (x+1)*scale, (y+1)*scale, outline=\"#fff\", fill=\"#000\")\n",
    "                for action in range(4):\n",
    "                    if not s in self.q_table:\n",
    "                        continue\n",
    "                    value = self.q_table[s][action]\n",
    "                    color = int( 255 * ( value - min_value ) / ( max_value - min_value ))\n",
    "                    color = max( min( color, 255 ), 0 )\n",
    "                    color_string = '#%02x%02x%02x' % (255-color, color, 0)\n",
    "                    self.canvas.create_oval( (x + action_positions[action][0] - action_radius ) *scale,\n",
    "                                             (y + action_positions[action][1] - action_radius ) *scale,\n",
    "                                             (x + action_positions[action][0] + action_radius ) *scale,\n",
    "                                             (y + action_positions[action][1] + action_radius ) *scale, \n",
    "                                             outline=color_string, fill=color_string )\n",
    "        if curr_x is not None and curr_y is not None:\n",
    "            self.canvas.create_oval( (curr_x + 0.5 - curr_radius ) * scale, \n",
    "                                     (curr_y + 0.5 - curr_radius ) * scale, \n",
    "                                     (curr_x + 0.5 + curr_radius ) * scale, \n",
    "                                     (curr_y + 0.5 + curr_radius ) * scale, \n",
    "                                     outline=\"#fff\", fill=\"#fff\" )\n",
    "        self.root.update()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR: Caught std::exception: unrecognised option '-f'\n",
      "\n",
      "Malmo version: 0.36.0\n",
      "\n",
      "Allowed options:\n",
      "  -h [ --help ]         show description of allowed options\n",
      "  --test                run this as an integration test\n",
      "\n",
      "\n",
      "Loading mission from ./maze2.xml\n",
      "\n",
      "Repeat 1 of 30\n",
      "Waiting for the mission to start .....\n",
      "Mission started \n",
      "--------------------------------\n",
      "Error: AgentHost::sendCommand : commands connection is not open. Is the mission running?\n",
      "Reward obtained:-742\n",
      "Total time trained: 300.6891858577728 seconds\n",
      "Number of actions performed: 743\n",
      "\n",
      "Repeat 2 of 30\n",
      "Waiting for the mission to start .....\n",
      "Mission started \n",
      "--------------------------------\n",
      "Error: AgentHost::sendCommand : commands connection is not open. Is the mission running?\n",
      "Reward obtained:-615\n",
      "Total time trained: 300.71309542655945 seconds\n",
      "Number of actions performed: 616\n",
      "\n",
      "Repeat 3 of 30\n",
      "Waiting for the mission to start .....\n",
      "Mission started \n",
      "--------------------------------\n",
      "Error: AgentHost::sendCommand : commands connection is not open. Is the mission running?\n",
      "Reward obtained:-578\n",
      "Total time trained: 300.57761240005493 seconds\n",
      "Number of actions performed: 579\n",
      "\n",
      "Repeat 4 of 30\n",
      "Waiting for the mission to start ....\n",
      "Mission started \n",
      "--------------------------------\n",
      "Error: AgentHost::sendCommand : commands connection is not open. Is the mission running?\n",
      "Reward obtained:-553\n",
      "Total time trained: 300.8546929359436 seconds\n",
      "Number of actions performed: 554\n",
      "\n",
      "Repeat 5 of 30\n",
      "Waiting for the mission to start .....\n",
      "Mission started \n",
      "--------------------------------\n",
      "Error: AgentHost::sendCommand : commands connection is not open. Is the mission running?\n",
      "Reward obtained:-540\n",
      "Total time trained: 300.71222138404846 seconds\n",
      "Number of actions performed: 541\n",
      "\n",
      "Repeat 6 of 30\n",
      "Waiting for the mission to start ....\n",
      "Mission started \n",
      "--------------------------------\n",
      "Error: AgentHost::sendCommand : commands connection is not open. Is the mission running?\n",
      "Reward obtained:11092\n",
      "Total time trained: 238.09638738632202 seconds\n",
      "Number of actions performed: 409\n",
      "\n",
      "Repeat 7 of 30\n",
      "Waiting for the mission to start ....\n",
      "Mission started \n",
      "--------------------------------\n",
      "Reward obtained:-444\n",
      "Total time trained: 300.7867078781128 seconds\n",
      "Number of actions performed: 444\n",
      "\n",
      "Repeat 8 of 30\n",
      "Waiting for the mission to start .....\n",
      "Mission started \n",
      "--------------------------------\n",
      "Error: AgentHost::sendCommand : commands connection is not open. Is the mission running?\n",
      "Reward obtained:9717\n",
      "Total time trained: 195.74471950531006 seconds\n",
      "Number of actions performed: 284\n",
      "\n",
      "Repeat 9 of 30\n",
      "Waiting for the mission to start ....\n",
      "Mission started \n",
      "--------------------------------\n",
      "Reward obtained:583\n",
      "Total time trained: 300.6799864768982 seconds\n",
      "Number of actions performed: 418\n",
      "\n",
      "Repeat 10 of 30\n",
      "Waiting for the mission to start ....\n",
      "Mission started \n",
      "--------------------------------\n",
      "Error: AgentHost::sendCommand : commands connection is not open. Is the mission running?\n",
      "Reward obtained:-417\n",
      "Total time trained: 300.9833941459656 seconds\n",
      "Number of actions performed: 418\n",
      "\n",
      "Repeat 11 of 30\n",
      "Waiting for the mission to start ....\n",
      "Mission started \n",
      "--------------------------------\n",
      "Error: AgentHost::sendCommand : commands connection is not open. Is the mission running?\n",
      "Reward obtained:601\n",
      "Total time trained: 301.3938093185425 seconds\n",
      "Number of actions performed: 400\n",
      "\n",
      "Repeat 12 of 30\n",
      "Waiting for the mission to start .....\n",
      "Mission started \n",
      "--------------------------------\n",
      "Error: AgentHost::sendCommand : commands connection is not open. Is the mission running?\n",
      "Reward obtained:-401\n",
      "Total time trained: 300.9127700328827 seconds\n",
      "Number of actions performed: 402\n",
      "\n",
      "Repeat 13 of 30\n",
      "Waiting for the mission to start ....\n",
      "Mission started \n",
      "--------------------------------\n",
      "Error: AgentHost::sendCommand : commands connection is not open. Is the mission running?\n",
      "Reward obtained:96\n",
      "Total time trained: 300.9227879047394 seconds\n",
      "Number of actions performed: 405\n",
      "\n",
      "Repeat 14 of 30\n",
      "Waiting for the mission to start ....\n",
      "Mission started \n",
      "--------------------------------\n",
      "Reward obtained:102\n",
      "Total time trained: 300.65178871154785 seconds\n",
      "Number of actions performed: 398\n",
      "\n",
      "Repeat 15 of 30\n",
      "Waiting for the mission to start ....\n",
      "Mission started \n",
      "--------------------------------\n",
      "Reward obtained:2110\n",
      "Total time trained: 300.71198749542236 seconds\n",
      "Number of actions performed: 391\n",
      "\n",
      "Repeat 16 of 30\n",
      "Waiting for the mission to start ....\n",
      "Mission started \n",
      "--------------------------------\n",
      "Error: AgentHost::sendCommand : commands connection is not open. Is the mission running?\n",
      "Reward obtained:118\n",
      "Total time trained: 301.0115418434143 seconds\n",
      "Number of actions performed: 383\n",
      "\n",
      "Repeat 17 of 30\n",
      "Waiting for the mission to start ....\n",
      "Mission started \n",
      "--------------------------------\n",
      "Reward obtained:1158\n",
      "Total time trained: 272.76756715774536 seconds\n",
      "Number of actions performed: 342\n",
      "\n",
      "Repeat 18 of 30\n",
      "Waiting for the mission to start ....\n",
      "Mission started \n",
      "--------------------------------\n",
      "Error: AgentHost::sendCommand : commands connection is not open. Is the mission running?\n",
      "Reward obtained:11753\n",
      "Total time trained: 205.49962186813354 seconds\n",
      "Number of actions performed: 248\n",
      "\n",
      "Repeat 19 of 30\n",
      "Waiting for the mission to start ....\n",
      "Mission started \n",
      "--------------------------------\n",
      "Error: AgentHost::sendCommand : commands connection is not open. Is the mission running?\n",
      "Reward obtained:10470\n",
      "Total time trained: 26.257941961288452 seconds\n",
      "Number of actions performed: 31\n",
      "\n",
      "Repeat 20 of 30\n",
      "Waiting for the mission to start .....\n",
      "Mission started \n",
      "--------------------------------\n",
      "Error: AgentHost::sendCommand : commands connection is not open. Is the mission running?\n",
      "Reward obtained:-359\n",
      "Total time trained: 301.0257546901703 seconds\n",
      "Number of actions performed: 360\n",
      "\n",
      "Repeat 21 of 30\n",
      "Waiting for the mission to start .....\n",
      "Mission started \n",
      "--------------------------------\n",
      "Error: AgentHost::sendCommand : commands connection is not open. Is the mission running?\n",
      "Reward obtained:650\n",
      "Total time trained: 300.884220123291 seconds\n",
      "Number of actions performed: 351\n",
      "\n",
      "Repeat 22 of 30\n",
      "Waiting for the mission to start ....\n",
      "Mission started \n",
      "--------------------------------\n",
      "Error: AgentHost::sendCommand : commands connection is not open. Is the mission running?\n",
      "Reward obtained:-352\n",
      "Total time trained: 301.272314786911 seconds\n",
      "Number of actions performed: 353\n",
      "\n",
      "Repeat 23 of 30\n",
      "Waiting for the mission to start ....\n",
      "Mission started \n",
      "--------------------------------\n",
      "Error: AgentHost::sendCommand : commands connection is not open. Is the mission running?\n",
      "Reward obtained:148\n",
      "Total time trained: 301.4056534767151 seconds\n",
      "Number of actions performed: 353\n",
      "\n",
      "Repeat 24 of 30\n",
      "Waiting for the mission to start ....\n",
      "Mission started \n",
      "--------------------------------\n",
      "Reward obtained:724\n",
      "Total time trained: 236.7443664073944 seconds\n",
      "Number of actions performed: 277\n",
      "\n",
      "Repeat 25 of 30\n",
      "Waiting for the mission to start ....\n",
      "Mission started \n",
      "--------------------------------\n",
      "Reward obtained:9882\n",
      "Total time trained: 109.45573902130127 seconds\n",
      "Number of actions performed: 119\n",
      "\n",
      "Repeat 26 of 30\n",
      "Waiting for the mission to start ....\n",
      "Mission started \n",
      "--------------------------------\n",
      "Error: AgentHost::sendCommand : commands connection is not open. Is the mission running?\n",
      "Reward obtained:655\n",
      "Total time trained: 301.2911820411682 seconds\n",
      "Number of actions performed: 346\n",
      "\n",
      "Repeat 27 of 30\n",
      "Waiting for the mission to start ....\n",
      "Mission started \n",
      "--------------------------------\n",
      "Reward obtained:-350\n",
      "Total time trained: 300.748836517334 seconds\n",
      "Number of actions performed: 350\n",
      "\n",
      "Repeat 28 of 30\n",
      "Waiting for the mission to start .....\n",
      "Mission started \n",
      "--------------------------------\n",
      "Error: AgentHost::sendCommand : commands connection is not open. Is the mission running?\n",
      "Reward obtained:165\n",
      "Total time trained: 301.0703091621399 seconds\n",
      "Number of actions performed: 336\n",
      "\n",
      "Repeat 29 of 30\n",
      "Waiting for the mission to start ....\n",
      "Mission started \n",
      "--------------------------------\n",
      "Reward obtained:-344\n",
      "Total time trained: 300.7283139228821 seconds\n",
      "Number of actions performed: 344\n",
      "\n",
      "Repeat 30 of 30\n",
      "Waiting for the mission to start .....\n",
      "Mission started \n",
      "--------------------------------\n",
      "Error: AgentHost::sendCommand : commands connection is not open. Is the mission running?\n",
      "Reward obtained:-347\n",
      "Total time trained: 301.09870052337646 seconds\n",
      "Number of actions performed: 348\n",
      "\n",
      "Done.\n",
      "Cumulative rewards for all 30 runs:\n",
      "[-742.0, -615.0, -578.0, -553.0, -540.0, 11092.0, -444.0, 9717.0, 583.0, -417.0, 601.0, -401.0, 96.0, 102.0, 2110.0, 118.0, 1158.0, 11753.0, 10470.0, -359.0, 650.0, -352.0, 148.0, 724.0, 9882.0, 655.0, -350.0, 165.0, -344.0, -347.0]\n",
      "Average reward: 1799.4\n",
      "Total time used for 30 trials: 8220.887239456177 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time_all_trials = time.time()\n",
    "results_df = []\n",
    "cumulative_rewards = []\n",
    "\n",
    "agent = QL_AEG()\n",
    "agent_host = MalmoPython.AgentHost()\n",
    "\n",
    "try:\n",
    "    agent_host.parse( sys.argv )\n",
    "except RuntimeError as e:\n",
    "    print('ERROR:',e)\n",
    "    print(agent_host.getUsage())\n",
    "    exit(1)\n",
    "if agent_host.receivedArgument(\"help\"):\n",
    "    print(agent_host.getUsage())\n",
    "    exit(0)\n",
    "\n",
    "mission_file = './maze2.xml'\n",
    "with open(mission_file, 'r') as f:\n",
    "    print(\"Loading mission from %s\" % mission_file)\n",
    "    mission_xml = f.read()\n",
    "    my_mission = MalmoPython.MissionSpec(mission_xml, True)\n",
    "    \n",
    "agent_host.setObservationsPolicy(MalmoPython.ObservationsPolicy.LATEST_OBSERVATION_ONLY)\n",
    "agent_host.setVideoPolicy(MalmoPython.VideoPolicy.LATEST_FRAME_ONLY)\n",
    "my_mission_record = MalmoPython.MissionRecordSpec()\n",
    "my_mission.requestVideo(800, 500)\n",
    "my_mission.setViewpoint(0)\n",
    "\n",
    "my_clients = MalmoPython.ClientPool()\n",
    "my_clients.add(MalmoPython.ClientInfo('127.0.0.1', 10000)) # add Minecraft machines here as available\n",
    "agentID = 0\n",
    "expID = 'Q-Learning with Alpha, Epsilon, and Gamma.'\n",
    "\n",
    "max_retries = 3\n",
    "\n",
    "if agent_host.receivedArgument(\"test\"):\n",
    "    num_repeats = 1\n",
    "else:\n",
    "    num_repeats = 30\n",
    "\n",
    "for i in range(num_repeats):\n",
    "    \n",
    "    print()\n",
    "    print('Repeat %d of %d' % (i+1, num_repeats ))\n",
    "    \n",
    "    for retry in range(max_retries):\n",
    "        try:\n",
    "            agent_host.startMission(my_mission, my_clients, my_mission_record, agentID, \"%s-%d\" % (expID, i))\n",
    "            break\n",
    "        except RuntimeError as e:\n",
    "            if retry == max_retries - 1:\n",
    "                print(\"Error starting mission:\",e)\n",
    "                exit(1)\n",
    "            else:\n",
    "                time.sleep(2.5)\n",
    "\n",
    "    print(\"Waiting for the mission to start\", end=' ')\n",
    "    world_state = agent_host.getWorldState()\n",
    "    while not world_state.has_mission_begun:\n",
    "        \n",
    "        print(\".\", end=\"\")\n",
    "        time.sleep(0.1)\n",
    "        world_state = agent_host.getWorldState()\n",
    "        for error in world_state.errors:\n",
    "            print()   \n",
    "    print()\n",
    "    \n",
    "    print(\"Mission started \")\n",
    "    print(\"--------------------------------\")\n",
    "    # Run the program.\n",
    "    start_time = time.time()\n",
    "    cumulative_reward = agent.run(agent_host)\n",
    "    print('Reward obtained:%d' % cumulative_reward)\n",
    "    cumulative_rewards += [cumulative_reward]\n",
    "    timeTaken = (time.time() - start_time)\n",
    "    print(\"Total time trained:\", \"%s seconds\" % timeTaken )\n",
    "    print(\"Number of actions performed:\", agent.count_action())\n",
    "    result = [i, cumulative_reward, timeTaken, agent.count_action()]\n",
    "    results_df.append(result)\n",
    "    agent.resetcount_action()\n",
    "\n",
    "print()\n",
    "print(\"Done.\")\n",
    "print(\"Cumulative rewards for all %d runs:\" % num_repeats)\n",
    "print(cumulative_rewards)\n",
    "print(\"Average reward:\", sum(cumulative_rewards)/num_repeats)\n",
    "print(\"Total time used for 30 trials:\", \"%s seconds\" % (time.time() - start_time_all_trials))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Number of attempt</th>\n",
       "      <th>Reward</th>\n",
       "      <th>Time taken</th>\n",
       "      <th>Number of actions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>-742.0</td>\n",
       "      <td>300.689186</td>\n",
       "      <td>743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>-615.0</td>\n",
       "      <td>300.713095</td>\n",
       "      <td>616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>-578.0</td>\n",
       "      <td>300.577612</td>\n",
       "      <td>579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>-553.0</td>\n",
       "      <td>300.854693</td>\n",
       "      <td>554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>-540.0</td>\n",
       "      <td>300.712221</td>\n",
       "      <td>541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>11092.0</td>\n",
       "      <td>238.096387</td>\n",
       "      <td>409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>-444.0</td>\n",
       "      <td>300.786708</td>\n",
       "      <td>444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>9717.0</td>\n",
       "      <td>195.744720</td>\n",
       "      <td>284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>583.0</td>\n",
       "      <td>300.679986</td>\n",
       "      <td>418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>-417.0</td>\n",
       "      <td>300.983394</td>\n",
       "      <td>418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>601.0</td>\n",
       "      <td>301.393809</td>\n",
       "      <td>400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>-401.0</td>\n",
       "      <td>300.912770</td>\n",
       "      <td>402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>96.0</td>\n",
       "      <td>300.922788</td>\n",
       "      <td>405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>102.0</td>\n",
       "      <td>300.651789</td>\n",
       "      <td>398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>2110.0</td>\n",
       "      <td>300.711987</td>\n",
       "      <td>391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>118.0</td>\n",
       "      <td>301.011542</td>\n",
       "      <td>383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>1158.0</td>\n",
       "      <td>272.767567</td>\n",
       "      <td>342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>11753.0</td>\n",
       "      <td>205.499622</td>\n",
       "      <td>248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>10470.0</td>\n",
       "      <td>26.257942</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19</td>\n",
       "      <td>-359.0</td>\n",
       "      <td>301.025755</td>\n",
       "      <td>360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>20</td>\n",
       "      <td>650.0</td>\n",
       "      <td>300.884220</td>\n",
       "      <td>351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>21</td>\n",
       "      <td>-352.0</td>\n",
       "      <td>301.272315</td>\n",
       "      <td>353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>22</td>\n",
       "      <td>148.0</td>\n",
       "      <td>301.405653</td>\n",
       "      <td>353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>23</td>\n",
       "      <td>724.0</td>\n",
       "      <td>236.744366</td>\n",
       "      <td>277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>24</td>\n",
       "      <td>9882.0</td>\n",
       "      <td>109.455739</td>\n",
       "      <td>119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>25</td>\n",
       "      <td>655.0</td>\n",
       "      <td>301.291182</td>\n",
       "      <td>346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>26</td>\n",
       "      <td>-350.0</td>\n",
       "      <td>300.748837</td>\n",
       "      <td>350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>27</td>\n",
       "      <td>165.0</td>\n",
       "      <td>301.070309</td>\n",
       "      <td>336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>28</td>\n",
       "      <td>-344.0</td>\n",
       "      <td>300.728314</td>\n",
       "      <td>344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>29</td>\n",
       "      <td>-347.0</td>\n",
       "      <td>301.098701</td>\n",
       "      <td>348</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Number of attempt   Reward  Time taken  Number of actions\n",
       "0                   0   -742.0  300.689186                743\n",
       "1                   1   -615.0  300.713095                616\n",
       "2                   2   -578.0  300.577612                579\n",
       "3                   3   -553.0  300.854693                554\n",
       "4                   4   -540.0  300.712221                541\n",
       "5                   5  11092.0  238.096387                409\n",
       "6                   6   -444.0  300.786708                444\n",
       "7                   7   9717.0  195.744720                284\n",
       "8                   8    583.0  300.679986                418\n",
       "9                   9   -417.0  300.983394                418\n",
       "10                 10    601.0  301.393809                400\n",
       "11                 11   -401.0  300.912770                402\n",
       "12                 12     96.0  300.922788                405\n",
       "13                 13    102.0  300.651789                398\n",
       "14                 14   2110.0  300.711987                391\n",
       "15                 15    118.0  301.011542                383\n",
       "16                 16   1158.0  272.767567                342\n",
       "17                 17  11753.0  205.499622                248\n",
       "18                 18  10470.0   26.257942                 31\n",
       "19                 19   -359.0  301.025755                360\n",
       "20                 20    650.0  300.884220                351\n",
       "21                 21   -352.0  301.272315                353\n",
       "22                 22    148.0  301.405653                353\n",
       "23                 23    724.0  236.744366                277\n",
       "24                 24   9882.0  109.455739                119\n",
       "25                 25    655.0  301.291182                346\n",
       "26                 26   -350.0  300.748837                350\n",
       "27                 27    165.0  301.070309                336\n",
       "28                 28   -344.0  300.728314                344\n",
       "29                 29   -347.0  301.098701                348"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "column = ['Number of attempt', 'Reward', 'Time taken', 'Number of actions']\n",
    "results_df = pd.DataFrame(results_df, columns = column)\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current epsilon value: 0.6\n",
      "Current alpha value: 0.1\n",
      "Current gamma value: 0.1\n"
     ]
    }
   ],
   "source": [
    "print(\"Current epsilon value:\", epsilon)\n",
    "print(\"Current alpha value:\", alpha)\n",
    "print(\"Current gamma value:\", gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.to_csv('E06A01G01_M2.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
